{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmordig/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import more_itertools\n",
    "import logging\n",
    "import sys\n",
    "import textwrap\n",
    "import collections\n",
    "\n",
    "\n",
    "import tqdm\n",
    "from transformers import pipeline, HfArgumentParser\n",
    "from datasets import load_from_disk\n",
    "import gradio as gr\n",
    "from contextlib import nullcontext, redirect_stdout\n",
    "\n",
    "if \"__file__\" in locals():\n",
    "    import os, sys; sys.path.insert(0, os.path.join(os.path.dirname(__file__), \"..\")) #todo\n",
    "else:\n",
    "    import os, sys; sys.path.insert(0, os.path.join(os.path.dirname(__vsc_ipynb_file__), \"..\")) #todo\n",
    "from question_answer_utils import extract_answer, extract_question_prompt, get_question_answer_to_chat_formatter\n",
    "from utils import load_model_for_inference, setup_logging, subset_dataset\n",
    "\n",
    "#%%\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "setup_logging()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/03/2024 11:37:51 - WARNING - __main__ - Using dummy args\n"
     ]
    }
   ],
   "source": [
    "logger.warning(\"Using dummy args\")\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2_2ex\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/training/overfit_single_nocompl/gpt2_withpeft\"\n",
    "model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/gpt2_1000ex_peftFalse\"\n",
    "# model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/gpt2_1000ex_peftTrue\" # broken\n",
    "model_name_or_path = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/Llama-2-7b-chat-hf_1000ex_peftTrue\"\n",
    "dataset_name = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/datasets/alpha_geo_small_processed\"\n",
    "# dataset_test_name = \"test\"\n",
    "dataset_test_name = \"train\" # for overfitting exp\n",
    "filename_predictions_out = \"/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/predictions/exp_small/gpt2_predictions.txt\"\n",
    "# max_predict_samples = 2\n",
    "max_predict_samples = 2\n",
    "dataset_text_field = \"text\"\n",
    "max_new_tokens = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/03/2024 11:37:52 - INFO - __main__ - Generating predictions, writing to file '/home/mmordig/reinforcement/alphageometry/LLM_finetuner/runs/verbalization/predictions/exp_small/gpt2_predictions.txt'\n",
      "05/03/2024 11:37:52 - INFO - utils - Loading model from '/home/mmordig/reinforcement/alphageometry/LLM_finetuner/cluster_runs/verbalization/training/failed_quote/gpt2_1000ex_peftTrue/checkpoint-3558'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mmordig/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/03/2024 11:37:52 - INFO - accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m raw_datasets[dataset_test_name]\n\u001b[1;32m      5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m subset_dataset(dataset, n_samples\u001b[38;5;241m=\u001b[39mmax_predict_samples)\n\u001b[0;32m----> 7\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_for_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m is_chat_model \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mchat_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_chat_model:\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/jupyter_notebooks/../utils.py:34\u001b[0m, in \u001b[0;36mload_model_for_inference\u001b[0;34m(model_checkpoints_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# bnb_config = None\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path) \u001b[38;5;66;03m# don't add eos_token since in generation mode\u001b[39;00m\n\u001b[1;32m     36\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# for auto-regressive generation\u001b[39;00m\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/transformers/modeling_utils.py:3742\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3739\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[1;32m   3741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3742\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3744\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3746\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   3750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/transformers/integrations/peft.py:206\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     processed_adapter_state_dict[new_key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_adapter_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incompatible_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# check only for unexpected keys\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(incompatible_keys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(incompatible_keys\u001b[38;5;241m.\u001b[39munexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/peft/utils/save_and_load.py:249\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m load_result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    251\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_encoder[adapter_name]\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    252\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m: peft_model_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]}, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     )\n",
      "File \u001b[0;32m~/reinforcement/alphageometry/LLM_finetuner/verbalization_venv3/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT2LMHeadModel:\n\tsize mismatch for transformer.wte.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([50258, 768]) from checkpoint, the shape in current model is torch.Size([50257, 768])."
     ]
    }
   ],
   "source": [
    "logger.info(f\"Generating predictions, writing to file '{filename_predictions_out}'\")\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_name)\n",
    "dataset = raw_datasets[dataset_test_name]\n",
    "dataset = subset_dataset(dataset, n_samples=max_predict_samples)\n",
    "\n",
    "model, tokenizer = load_model_for_inference(model_name_or_path)\n",
    "\n",
    "is_chat_model = tokenizer.chat_template is not None\n",
    "if is_chat_model:\n",
    "    logger.info(\"Detected chat model, formatting according to chat template\")\n",
    "    # assumes user-assistant roles\n",
    "    prompt_extraction_function = get_question_answer_to_chat_formatter(tokenizer, text_column=None, add_generation_prompt=True)\n",
    "else:\n",
    "    prompt_extraction_function = extract_question_prompt\n",
    "    \n",
    "def extract_extra_cols(batch):\n",
    "    return {\n",
    "        \"question_prompt\": [prompt_extraction_function(item) for item in batch[dataset_text_field]],\n",
    "        \"answer_only\": [extract_answer(item) for item in batch[dataset_text_field]],\n",
    "    }\n",
    "dataset = dataset.map(extract_extra_cols, batched=True)\n",
    "logger.info(f\"Example datapoint: {dataset[0]}\")\n",
    "\n",
    "# use_cache to avoid recomputing hidden states, see https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958\n",
    "# max_new_tokens = 70\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, \n",
    "    num_return_sequences=2, num_beams=4, do_sample=True, max_new_tokens=max_new_tokens, use_cache=True,\n",
    "    return_full_text=False, # answer only\n",
    "    num_workers=2, \n",
    "    # batch_size=2 # triggers a cuda device-side error, maybe related to https://github.com/huggingface/transformers/issues/22546\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# model_name_or_path = \"gpt2\"\n",
    "# model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False, add_eos_token=True, pad_token=\"[PAD]\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, add_eos_token=True)\n",
    "# tokenizer.pad_token = \"[PAD]\"\n",
    "# print(tokenizer.eos_token)\n",
    "# print(tokenizer.pad_token)\n",
    "\n",
    "# # ?tokenizer\n",
    "# tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_prompt = dataset[0][\"question_prompt\"]\n",
    "# text = dataset[0][\"text\"]\n",
    "# tokenizer.decode(tokenizer(text)[\"input_ids\"], include_special_tokens=True)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 714.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Query: \n",
      "### Question: Points B, A, C are defined such that triangle ABC is an⏎\n",
      "equilateral triangle. Define points D, F, & E such that E, D, and F is⏎\n",
      "a right angle isosceles triangle with the right angle at D. Point G is⏎\n",
      "defined such that G, A, E are three consecutive vertices of a square.⏎\n",
      "Circle centered at C with radius CE intersects circle centered at G⏎\n",
      "with radius GE at H and E. Points I and J are defined such that line⏎\n",
      "IC and line JC are the two tangents to circle centered at B with⏎\n",
      "radius BF at point I and J respectively.. Define point K such that⏎\n",
      "line DI and line AG are parallel. line JK perpendicular to line DI.⏎\n",
      "line EK perpendicular to line AG. line JK meets line EK at the point⏎\n",
      "K. ### Answer:\n",
      "Expected answer: \n",
      "A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H⏎\n",
      "= intersection_cc H C G E; I J = tangent I J C B F; K =⏎\n",
      "intersection_tt K J D I E A G\n",
      "Number of candidates that are equal to expected: 0\n",
      "Number of candidates that begin with expected: 0\n",
      "#################### Candidate 1 (appears 2 times) ####################\n",
      "A B C = ieq_triangle A B C; D E F = risos D E F; G = psquare G A E; H⏎\n",
      "= intersection_cc H C G E; I J = tangent I J C B F; K =⏎\n",
      "intersection_lp K D I G J A; L M N = 3pe <MAX token length exceeded>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "break_nicely = lambda x: \"\\u23CE\\n\".join(textwrap.wrap(x)) # symbol \"⏎\" for line breaks\n",
    "# using a pipe/dataset is faster because GPU works in the background while writing to file\n",
    "# with open(filename_predictions_out, 'w') as f, redirect_stdout(f):\n",
    "# with nullcontext():\n",
    "for (out, question_prompt, gt_answer) in tqdm.tqdm(zip(pipe(dataset[\"question_prompt\"]), dataset[\"question_prompt\"], dataset[\"answer_only\"])):\n",
    "    print(\"#\"*80)\n",
    "    print(\"Query: \")\n",
    "    print(break_nicely(question_prompt))\n",
    "    print(\"Expected answer: \")\n",
    "    print(break_nicely(gt_answer))\n",
    "    # strips whitespace because generated text has leading and trailing whitespace\n",
    "    out_counted = collections.Counter([candidate[\"generated_text\"].strip() for candidate in out])\n",
    "    gt_answer = gt_answer.strip()\n",
    "    print(f\"Number of candidates that are equal to expected: {out_counted.get(gt_answer, 0)}\")\n",
    "    print(f\"Number of candidates that begin with expected:\", sum(out_counted[key] for key in out_counted if key.startswith(gt_answer)))\n",
    "    # for (i, candidate) in enumerate(out):\n",
    "    #     candidate_text = candidate[\"generated_text\"]\n",
    "    for (i, (candidate_text, count)) in enumerate(out_counted.items()):\n",
    "        # logger.info(f\"Generated text: {candidate_text}\")\n",
    "        # answer = extract_answer(candidate_text)\n",
    "        answer = candidate_text\n",
    "        print(\"#\"*20 + f\" Candidate {i+1} (appears {count} times) \" + \"#\"*20)\n",
    "        extra = \"\"\n",
    "        # not perfect because tokenizing with question_prompt may lead to different tokenization\n",
    "        if len(tokenizer(answer)[\"input_ids\"]) == max_new_tokens:\n",
    "            extra = \" <MAX token length exceeded>\"\n",
    "        print(break_nicely(answer) + extra)\n",
    "    # sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(out_counted.keys())[0])\n",
    "print(' ' + gt_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out_counted.keys())[0] == (' ' + gt_answer)\n",
    "def show_diff(s1, s2):\n",
    "    for (c1, c2) in zip(s1, s2):\n",
    "        if c1 != c2:\n",
    "            print(c1, c2)\n",
    "show_diff(list(out_counted.keys())[0], (' ' + gt_answer))\n",
    "\n",
    "s1 = list(out_counted.keys())[0]\n",
    "s2 = (' ' + gt_answer)\n",
    "s1[len(s2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0][\"text\"]\n",
    "question_prompt = \"### Question: Points B, A, C are defined such that triangle ABC is an equilateral triangle. Define points D, F, & E such that E, D, and F is a right angle isosceles triangle with the right angle at D. Point G is defined such that G, A, E are three consecutive vertices of a square. Circle centered at C with radius CE intersects circle centered at G with radius GE at H and E. Points I and J are defined such that line IC and line JC are the two tangents to circle centered at B with radius BF at point I and J respectively.. Define point K such that line DI and line AG are parallel. line JK perpendicular to line DI. line EK perpendicular to line AG. line JK meets line EK at the point K. ### Answer:\"\n",
    "print(tokenizer(question_prompt + \" \")[\"input_ids\"][-10:] + tokenizer(\"answer\")[\"input_ids\"])\n",
    "print(tokenizer(question_prompt + \" answer\")[\"input_ids\"][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset_text_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"Hello answer\")[\"input_ids\"])\n",
    "print(tokenizer(\"Hello answer \")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.generate(**tokenizer(\"Hello answer\", return_tensors=\"pt\").to(\"cuda\")))\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"How are\", return_tensors=\"pt\").to(\"cuda\"), do_sample=False, max_new_tokens=20)[0]))\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"How are \", return_tensors=\"pt\").to(\"cuda\"), do_sample=False, max_new_tokens=20)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verbalization_venv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
